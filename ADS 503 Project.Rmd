---
title: "Liver Disease"
author: "Travis Lloyd"
date: '2022-06-24'
output: html_document
---

---
title: "R Notebook"
output: html_notebook
---

## Loading packages

```{r}
library(AppliedPredictiveModeling)
library (tidyverse)
library(psych)
library(rpart)
library (caret)
library(pamr)
library(rpart.plot)
library(pROC)
library(MLmetrics)
library(GGally)
library(imputeMissings)
```


Data Set Information:

This data set contains 416 liver patient records and 167 non liver patient records.The data set was collected from north east of Andhra Pradesh, India. Selector is a class label used to divide into groups(liver patient or not). This data set contains 441 male patient records and 142 female patient records.

Any patient whose age exceeded 89 is listed as being of age "90".


Attribute Information:

1. Age Age of the patient
2. Gender Gender of the patient
3. TB Total Bilirubin
4. DB Direct Bilirubin
5. Alkphos Alkaline Phosphotase
6. Sgpt Alamine Aminotransferase
7. Sgot Aspartate Aminotransferase
8. TP Total Protiens
9. ALB Albumin
10. A/G Ratio Albumin and Globulin Ratio
11. Selector field used to split the data into two sets (labeled by the experts)


## Loading dataset

```{r}
ilpd <- read.csv("Indian Liver Patient Dataset (ILPD).csv",
               header = FALSE)
colnames(ilpd) = c("age", "gender", "tb", "db", "alkphos", "sgpt", "sgot", "tp", "alb", "ag_ratio", "selector")
ilpd$selector <- ifelse(ilpd$selector == 1, "Patient", "NonPatient")
str(ilpd)
```


## EDA (graphical and non-graphical representations of relationships between response variable and predictor variables)

```{r}
glimpse(ilpd)
describeBy(ilpd, "selector")
```

```{r}
g1 <- ggpairs(data=ilpd, title="Indian Liver Patient Dataset",
  mapping=ggplot2::aes(colour = as.factor(selector), alpha=0.5),
  lower=list(combo=wrap("facethist",binwidth=1)))
g1
```


## Data wrangling and pre-processing (handling of missing values, outliers, correlated features, etc.)

```{r}
# Creating a Dummy Variable and converting the Outcome in a factor
df <- ilpd %>% 
  mutate(male = ifelse(gender == "Male", 1, 0), .after=gender) %>%
  mutate(selector = as.factor(selector)) %>% 
  select(-gender)
# Zero- and Near Zero-Variance Predictors
who = nearZeroVar(df)
if(length(who) > 0) df <- df[,-who]
# Identifying Correlated Predictors
who = df %>% 
  select(-selector) %>% 
  cor(use="complete.obs") %>% 
  findCorrelation(cutoff=.75)
  
if(length(who) > 0) df <- df[,-who]
# Linear Dependencies
who = df %>% 
  select(-selector) %>% 
  cov(use="complete.obs") %>% 
  findLinearCombos()
if(length(who$remove) > 0) df <- df[,-who$remove]

# Removing Influential Obs by Cooks distance
mod <- glm(selector ~ ., data=df, family="binomial")
cooksd <- cooks.distance(mod)
who <- cooksd > 4*mean(cooksd, na.rm=T)
if(sum(who) > 0) 
  df <- df[-who,]

selector <- df[ ,ncol(df)]
df <- df[,1:ncol(df)-1]
```


## Data splitting (training, validation, and test sets)

```{r}
set.seed(123)
## Partition the data with stratified sampling
training <- createDataPartition(selector, p = .8, list = FALSE)
## Partition train and test sets
df_train <- df[training, ]
df_test <- df[-training, ]
## Partition for the train and test sets for the response variable
selector_train <- selector[training]
selector_test <- selector[-training]

## Impute the missing data using knn method built into caret package
trainimp <- preProcess(df_train, "knnImpute")
df_train <- predict(trainimp, df_train)
df_test <- predict(trainimp, df_test)

summary(trainpr)
summary(testpr)

## Set training control for model building
ctrl <- trainControl(method = "repeatedcv", 10, 
                     repeats = 10, 
                     summaryFunction = twoClassSummary, 
                     classProbs = TRUE, 
                     savePredictions = TRUE)
```

KNN Model

```{r}
set.seed(476)
knnFit <- train(x = df_train, 
               y = selector_train,
               method = "knn",
               metric = "ROC",
               trControl = ctrl)
knnFit
knnFit$finalModel
```

## Validation and testing (model tuning and evaluation)

```{r}
# we will make a grid of values to test in cross-validation.
knnGrid <-  expand.grid(k = c(1:50))
set.seed(476)
knnFitTune <- train(x = df_train, 
               y = selector_train,
               method = "knn",
               metric = "ROC",
               tuneGrid = knnGrid,
               trControl = ctrl)
knnFitTune
```

```{r}
plot(knnFitTune)
```

```{r}
knnFitTune$finalModel
```


## Results and final model selection (performance measures, etc.)

```{r}
knnCM <- confusionMatrix(knnFitTune, norm = "none")
knnCM
## Plot the ROC curve for the hold-out set
knnRoc <- roc(response = knnFitTune$pred$obs,
             predictor = knnFitTune$pred$Patient,
             levels = levels(knnFitTune$pred$obs))
```

```{r}
plot(knnRoc, legacy.axes = TRUE)
knnRoc$auc
```

```{r}
knnImp <- varImp(knnFitTune, scale = FALSE)
plot(knnImp)
```

## Score Test Data

```{r}
# let's score the test set using this model
pred_class <- predict(knnFitTune, df_test,'raw')
probs <- predict(knnFitTune, df_test,'prob')
# Merge original scaled test dataset with predicions
df_test.scored <- cbind(df[-training,], pred_class, probs)
glimpse(df_test.scored)
```


Logistic Regression Model

```{r, warning=FALSE}
set.seed(476)
lrFit <- train(x = df_train,
              y = selector_train,
              method = "glm",
              metric = "ROC",
              trControl = ctrl)
lrFit
lrFit$finalModel
```

## Validation and testing (model tuning and evaluation)

```{r, warning=FALSE}
# we will make a grid of values to test in cross-validation.
lrGrid <-  expand.grid(parameter = c(1:50))
set.seed(476)
lrFitTune <- train(x = df_train, 
               y = selector_train,
               method = "glm",
               metric = "ROC",
               tuneGrid = lrGrid,
               trControl = ctrl)
lrFitTune
```

## LDA Model
```{r, warning=FALSE}
# we will make a grid of values to test in cross-validation.
set.seed(476)
ldaFit <- train(x = df_train,
              y = selector_train,
              method = "lda",
              preProc = c("center","scale"),
              metric = "ROC",
              trControl = ctrl)
ldaFit
ldaFit$finalModel
```

```{r, warning=FALSE}
# we will make a grid of values to test in cross-validation.
set.seed(476)
ldaFitTune <- train(x = df_train, 
               y = selector_train,
               method = "lda",
               preProc = c("center","scale"),
               metric = "ROC",
               trControl = ctrl)
ldaFitTune
```

```{r}
ldaCM <- confusionMatrix(ldaFit, mode = prec_recall, norm = "none")
ldaCM

ldaRoc <- roc(response = ldaFit$pred$obs,
              predictor = ldaFit$pred$Patient,
              levels = rev(levels(ldaFit$pred$obs)))

plot(ldaRoc,legacy.axes = TRUE)

```
Random Forest - Bagging
```{r}
set.seed(476)
rfFit <- train(x = df_train, 
               y = selector_train,
               method = "rf",
               metric = "ROC",
               trControl = ctrl)
rfFit
rfFit$finalModel
```

## Validation and testing (model tuning and evaluation)

```{r}
# we will make a grid of values to test in cross-validation.
rfGrid <-  expand.grid(.mtry = c(1:10))
set.seed(476)
rfFitTune <- train(x = df_train, 
               y = selector_train,
               method = "rf",
               metric = "ROC",
               tuneGrid = rfGrid,
               trControl = ctrl)
rfFitTune
plot(rfFitTune)
rfFitTune$finalModel
```

## Evaluating the Model
```{r}
prediction <- predict(rfFitTune,df_test)
confusionMatrix(prediction,selector_test)
```

C5.0 - Boosting
```{r, warning = FALSE, message = FALSE}
set.seed(476)
CFit <- train(x = df_train, 
               y = selector_train,
               method = "C5.0",
               metric = "ROC",
               trControl = ctrl)
CFit
CFit$finalModel
```

## Validation and testing (model tuning and evaluation)

```{r, warning = FALSE, message = FALSE}
# we will make a grid of values to test in cross-validation.
CGrid <-  expand.grid(.trials= c(1,5,10,15,20), .winnow= c(TRUE,FALSE), .model= "tree")
set.seed(476)
CFitTune <- train(x = df_train, 
               y = selector_train,
               method = "C5.0",
               metric = "ROC",
               tuneGrid = CGrid,
               trControl = ctrl)
CFitTune
plot(CFitTune)
CFitTune$finalModel
```

## Evaluating the Model
```{r}
predictions <- predict(CFitTune,df_test)
confusionMatrix(predictions,selector_test)
```

## XGBoost Model
```{r, warning=FALSE}
# we will make a grid of values to test in cross-validation.
set.seed(476)
xgbFit <- train(x = df_train,
              y = selector_train,
              method = "xgbTree",
              metric = "ROC",
              trControl = ctrl)
xgbFit
xgbFit$finalModel
```

```{r, warning=FALSE}
# we will make a grid of values to test in cross-validation.
set.seed(476)
xgbFitTune <- train(x = df_train, 
               y = selector_train,
               method = "xgbTree",
               objective = "reg:squarederror",
               trControl = trainControl(method = "repeatedcv",
                                        number = 3,
                                        repeats = 2,
                                        verboseIter = TRUE),
               tune.grid = expand.grid(nrounds = c(500, 1000, 1500),
                                       eta = c(0.1, 0.5),
                                       colsample_bytree = c(0.5,1),
                                       subsample = c(0.5,1),
                                       gamma = c(0,50),
                                       min_child_weight = c(0,20)))


```

```{r}
xgtest <- predict(xgbFitTune, df_test)
xgtest <- as.numeric(as.character(xgtest))
selector_test <- as.numeric(as.character(selector_test))
str(xgtest)
testerror <- xgtest - selector_test
mean(testerror)
sqrt(mean(testerror^2))
```


```{r}
xgbCM <- confusionMatrix(xgbFit, mode = prec_recall, norm = "none")
xgbCM

xgbRoc <- roc(response = xgbFit$pred$obs,
              predictor = xgbFit$pred$Patient,
              levels = rev(levels(xgbFit$pred$obs)))

plot(xgbRoc,legacy.axes = TRUE)

```




