library(AppliedPredictiveModeling)
library (tidyverse)
library(psych)
library(rpart)
library (caret)
library(pamr)
library(rpart.plot)
library(pROC)
library(MLmetrics)
library(GGally)
library(imputeMissings)
library(AppliedPredictiveModeling)
library (tidyverse)
library(psych)
library(rpart)
library (caret)
library(pamr)
library(rpart.plot)
library(pROC)
library(MLmetrics)
library(GGally)
library(imputeMissings)
ilpd <- read.csv("Indian Liver Patient Dataset (ILPD).csv",
header = FALSE)
colnames(ilpd) = c("age", "gender", "tb", "db", "alkphos", "sgpt", "sgot", "tp", "alb", "ag_ratio", "selector")
glimpse(ilpd)
describeBy(ilpd, "selector")
g1 <- ggpairs(data=ilpd, title="Indian Liver Patient Dataset",
mapping=ggplot2::aes(colour = as.factor(selector), alpha=0.5),
lower=list(combo=wrap("facethist",binwidth=1)))
g1
# Creating a Dummy Variable and converting the Outcome in a factor
df <- ilpd %>%
mutate(male = ifelse(gender == "Male", 1, 0), .after=gender) %>%
mutate(selector = as.factor(paste0("G",selector))) %>%
select(-gender)
# Zero- and Near Zero-Variance Predictors
who = nearZeroVar(df)
if(length(who) > 0) df <- df[,-who]
# Identifying Correlated Predictors
who = df %>%
select(-selector) %>%
cor(use="complete.obs") %>%
findCorrelation(cutoff=.75)
if(length(who) > 0) df <- df[,-who]
# Linear Dependencies
who = df %>%
select(-selector) %>%
cov(use="complete.obs") %>%
findLinearCombos()
if(length(who$remove) > 0) df <- df[,-who$remove]
# Imputation for missing values
df <- impute(df)
# Removing Influential Obs by Cooks distance
mod <- glm(selector ~ ., data=df, family="binomial")
cooksd <- cooks.distance(mod)
who <- cooksd > 4*mean(cooksd, na.rm=T)
if(sum(who) > 0) df <- df[-who,]
# Centering and scaling
df_clean <- df %>%
select(-selector) %>%
scale() %>%
as.data.frame()
# The outcome
selector = df$selector
set.seed(123)
## Partition the data with stratified sampling
training <- createDataPartition(selector, p = .8, list = FALSE)
## Partition train and test sets
df_train <- df_clean[training, ]
df_test <- df_clean[-training, ]
## Partition for the train and test sets for the response variable
selector_train <- selector[training]
selector_test <- selector[-training]
## Set training control for model building
ctrl <- trainControl(method = "repeatedcv", 10,
repeats = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = TRUE)
set.seed(476)
knnFit <- train(x = df_train,
y = selector_train,
method = "knn",
metric = "ROC",
trControl = ctrl)
knnFit
knnFit$finalModel
set.seed(476)
set.seed(476)
knnFit <- train(x = df_train,
y = selector_train,
method = "knn",
metric = "ROC",
trControl = ctrl)
knnFit
knnFit$finalModel
set.seed(476)
lrFit <- train(x = df_train,
y = selector_train,
method = "glm",
metric = "ROC",
trControl = ctrl)
lrFit
lrFit$finalModel
knnFit
# we will make a grid of values to test in cross-validation.
knnGrid <-  expand.grid(k = c(1:50))
set.seed(476)
knnFitTune <- train(x = df_train,
y = selector_train,
method = "knn",
metric = "ROC",
tuneGrid = knnGrid,
trControl = ctrl)
knnFitTune
# we will make a grid of values to test in cross-validation.
knnGrid <-  expand.grid(k = c(1:50))
# we will make a grid of values to test in cross-validation.
knnGrid <-  expand.grid(k = c(1:50))
set.seed(476)
knnFitTune <- train(x = df_train,
y = selector_train,
method = "knn",
metric = "ROC",
tuneGrid = knnGrid,
trControl = ctrl)
knnFitTune
# we will make a grid of values to test in cross-validation.
lrGrid <-  expand.grid(parameter = c(1:50))
set.seed(476)
lrFitTune <- train(x = df_train,
y = selector_train,
method = "glm",
metric = "ROC",
tuneGrid = lrGrid,
trControl = ctrl)
lrFitTune
